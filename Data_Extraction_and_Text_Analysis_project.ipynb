{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMAESsYRdc3LLzUIVJ0Nfgh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#import necessary pacakages\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import numpy as np\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re\n","import chardet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iVDevpM6zKf","executionInfo":{"status":"ok","timestamp":1710189095570,"user_tz":-330,"elapsed":407,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"b0139c13-14e8-43ca-b8a8-b98133f12207"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd '/gdrive/MyDrive/Colab Notebooks/ Data_Extraction_and_Text_Analysis_.ipynb'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Z6_qqoADYmC","executionInfo":{"status":"ok","timestamp":1710182268240,"user_tz":-330,"elapsed":3617,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"cb8a16e2-a839-4ac3-bcfa-bdee104fa014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","[Errno 2] No such file or directory: '/gdrive/MyDrive/Colab Notebooks/ Data_Extraction_and_Text_Analysis_.ipynb'\n","/content\n"]}]},{"cell_type":"code","source":["#read the url file into the pandas object\n","df = pd.read_excel('/gdrive/MyDrive/Input.xlsx')\n","\n","#loop throgh each row in the df\n","for index, row in df.iterrows():\n","  url = row['URL']\n","  url_id = row['URL_ID']\n","\n"," # make a request to url\n","  header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n","  try:\n","    response = requests.get(url,headers=header)\n","  except:\n","    print(\"can't get response of {}\".format(url_id))\n","\n","     #create a beautifulsoup object\n","  try:\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","  except:\n","    print(\"can't get page of {}\".format(url_id))\n","\n","      #find title\n","  try:\n","    title = soup.find('h1').get_text()\n","  except:\n","    print(\"can't get title of {}\".format(url_id))\n","    continue\n","  #find text\n","  article = \"\"\n","  try:\n","    for p in soup.find_all('p'):\n","      article += p.get_text()\n","  except:\n","    print(\"can't get text of {}\".format(url_id))\n","\n","     #write title and text to the file\n","  file_name = '/gdrive/MyDrive/TitleText' + str(url_id) + '.txt'\n","  with open(file_name, 'w') as file:\n","    file.write(title + '\\n' + article)"],"metadata":{"id":"6JA98gT7pHo5","executionInfo":{"status":"ok","timestamp":1710182350185,"user_tz":-330,"elapsed":78878,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b6900e8-4255-49fc-f8df-33e10a2be517"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["can't get title of 44\n","can't get title of 57\n","can't get title of 144\n"]}]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlcrFU-2xEwp","executionInfo":{"status":"ok","timestamp":1710182366150,"user_tz":-330,"elapsed":6530,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"12670786-b629-48b7-aa96-d4cab8eefc2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}]},{"cell_type":"code","source":["!pip install os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n85jKTEwLvQO","executionInfo":{"status":"ok","timestamp":1710188609914,"user_tz":-330,"elapsed":1801,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"1c78e31a-5252-4d03-d8c4-8481ad54ef69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Directories\n","text_dir = \"/gdrive/MyDrive/TitleText\"\n","stopwords_dir = \"/gdrive/MyDrive/StopWords\"\n","sentment_dir = \"/gdrive/MyDrive/MasterDictionary\"\n","\n","# load all stop wors from the stopwords directory and store in the set variable\n","stop_words = set()\n","for files in os.listdir(stopwords_dir):\n","  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n","    stop_words.update(set(f.read().splitlines()))"],"metadata":{"id":"3Nk3cB8T-CdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# store positive, Negative words from the directory\n","pos=set()\n","neg=set()\n"],"metadata":{"id":"pn9OjXWJ_rq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for files in os.listdir(sentment_dir):\n","  if files =='positive-words.txt':\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      pos.update(f.read().splitlines())\n","  else:\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      neg.update(f.read().splitlines())"],"metadata":{"id":"DtrdR_FX_yTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now collect the positive  and negative words from each file\n","# calculate the scores from the positive and negative words\n","positive_words = []\n","Negative_words =[]\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []"],"metadata":{"id":"PyY5EMV1_13b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#iterate through the list of docs\n","for i in range(len(docs)):\n","  positive_words.append([word for word in docs[i] if word.lower() in pos])\n","  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n","  positive_score.append(len(positive_words[i]))\n","  negative_score.append(len(Negative_words[i]))\n","  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n","  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))"],"metadata":{"id":"TEO-LJXm_5-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Average Sentence Length = the number of words / the number of sentences\n","# Percentage of Complex words = the number of complex words / the number of words\n","# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","\n","avg_sentence_length = []\n","Percentage_of_Complex_words  =  []\n","Fog_Index = []\n","complex_word_count =  []\n","avg_syllable_word_count =[]\n","\n","stopwords = set(stopwords.words('english'))\n","def measure(file):\n","  with open(os.path.join(text_dir, file),'r') as f:\n","    text = f.read()"],"metadata":{"id":"YXei1SAe_9_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/gdrive/MyDrive/TitleText\", \"r\") as file:\n","    text = file.read()"],"metadata":{"id":"4CBUNJx0AFOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove punctuations\n","text = re.sub(r'[^\\w\\s.]','',text)\n","\n","# split the given text file into sentences\n","sentences = text.split('.')\n","\n","# total number of sentences in a file\n","num_sentences = len(sentences)\n","\n","# total words in the file\n","words = [word  for word in text.split() if word.lower() not in stopwords ]\n","num_words = len(words)"],"metadata":{"id":"-CmHJCZbA69o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# complex words having syllable count is greater than 2\n","# Complex words are words in the text that contain more than two syllables.\n","complex_words = []\n","for word in words:\n","    vowels = 'aeiou'\n","    syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","    if syllable_count_word > 2:\n","        complex_words.append(word)"],"metadata":{"id":"GFBOf91DA8qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install autocorrect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82OxI0iFCkqX","executionInfo":{"status":"ok","timestamp":1710188257611,"user_tz":-330,"elapsed":6728,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"60addafc-7b6b-4ff7-af82-f941f42a07c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: autocorrect in /usr/local/lib/python3.10/dist-packages (2.6.1)\n"]}]},{"cell_type":"code","source":["# Syllable Count Per Word\n","# We count the number of Syllables in each word of the text by counting the vowels present in each word.\n","#  We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n","syllable_count = 0\n","syllable_words =[]\n","for word in words:\n","  if word.endswith('es'):\n","    word = word[:-2]\n","  elif word.endswith('ed'):\n","    word = word[:-2]\n","  vowels = 'aeiou'\n","  syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n","  if syllable_count_word >= 1:\n","    syllable_words.append(word)\n","    syllable_count += syllable_count_word\n","\n","\n","avg_sentence_length = float(avg_sentence_len)\n","\n","percent_complex_words = 0\n","\n","if len(syllable_words) > 0:\n","    avg_syllable_word_count = syllable_count / len(syllable_words)\n","else:\n","    avg_syllable_word_count = 0\n","\n","if num_words > 0:\n","    Percent_Complex_words  =  len(complex_words) / num_words\n","else:\n","    Percent_Complex_words = 0\n","Fog_Index = 0.4 * ((float(avg_sentence_length)) + percent_complex_words)\n","\n","#return avg_sentence_length, percent_complex_words, Fog_Index, len(complex_words), int(avg_syllable_word_count)"],"metadata":{"id":"8NJxOsPZBWTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYCqqpAUHLy1","executionInfo":{"status":"ok","timestamp":1710188267148,"user_tz":-330,"elapsed":4,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"db7d45e9-08a7-4fa7-c26f-8005f5625661"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":179}]},{"cell_type":"code","source":["!pip show chardet\n","!pip install chardet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jQiYABOOanJ","executionInfo":{"status":"ok","timestamp":1710189347232,"user_tz":-330,"elapsed":27937,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"26a2abe3-f840-452c-c14c-9e1c7e0842b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: chardet\n","Version: 5.2.0\n","Summary: Universal encoding detector for Python 3\n","Home-page: https://github.com/chardet/chardet\n","Author: Mark Pilgrim\n","Author-email: mark@diveintomark.org\n","License: LGPL\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: \n","Required-by: music21\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"]}]},{"cell_type":"code","source":["import glob\n","\n","for file in glob.glob(os.path.join(text_dir, \"*.txt\")):\n","\n","        x, y, z, a, b = measure(text)\n","        avg_sentence_length.append(x)\n","        Percentage_of_Complex_words.append(y)\n","        Fog_Index.append(z)\n","        complex_word_count.append(a)\n","        avg_syllable_word_count.append(b)"],"metadata":{"id":"L8Ej5Z4HNvFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n","# We count the total cleaned words present in the text by\n","# removing the stop words (using stopwords class of nltk package).\n","# removing any punctuations like ? ! , . from the word before counting.\n","\n","def cleaned_words(file):\n","    with open(os.path.join(text_dir, file), 'r', encoding='latin-1') as f:\n","        text = f.read()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        words = [word for word in text.split() if word.lower() not in stopwords]\n","        length = sum(len(word) for word in words)\n","        average_word_length = length / len(words)\n","    return len(words), average_word_length\n"],"metadata":{"id":"Oc6H8ZlcU11p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_count = []\n","average_word_length = []\n","for file in os.listdir(text_dir):\n","\n","  x = word_count\n","  y = average_word_length\n","  #x, y = cleaned_words(file)\n","  word_count.append(x)\n","  average_word_length.append(y)\n","\n","# To calculate Personal Pronouns mentioned in the text, we use regex to find\n","# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n","#  so that the country name US is not included in the list.\n","\n","  for file in os.listdir(text_dir):\n","    if os.path.isfile(os.path.join(text_dir, file)):\n","        with open(os.path.join(text_dir, file), 'rb') as f:\n","            bytes = f.read()\n","            encoding = chardet.detect(bytes)[\"encoding\"]\n","            if encoding is None:\n","                encoding = \"utf-8\"  # Set a default encoding if chardet fails\n","            try:\n","                text = bytes.decode(encoding)\n","            except UnicodeDecodeError:\n","                text = bytes.decode(\"latin-1\")\n","            personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","            count = 0\n","            for pronoun in personal_pronouns:\n","                count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text))  # \\b is used to match word boundaries\n","            pp_count.append(count)\n"],"metadata":{"id":"XqrcMII-WGOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_df = pd.read_excel('/gdrive/MyDrive/Output Data Structure.xlsx')"],"metadata":{"id":"3kq4rk6KYh3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# URL_ID 44 ,57, 144 does not exists i,e. page does not exist, throughs 404 error\n","# so we are going to drop these rows from the table\n","output_df.drop([44-37,57-37,144-37], axis = 0, inplace=True)"],"metadata":{"id":"QLQA2e9EYutf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(variables))\n","print(output_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExphV9KrZZAR","executionInfo":{"status":"ok","timestamp":1710192172470,"user_tz":-330,"elapsed":7,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}},"outputId":"9d20c9ef-ba9f-4812-8124-7a18491039a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13\n","(111, 15)\n"]}]},{"cell_type":"code","source":["# These are the required parameters\n","variables = [positive_score,\n","            negative_score,\n","            polarity_score,\n","            subjectivity_score,\n","            avg_sentence_length,\n","            Percentage_of_Complex_words,\n","            Fog_Index,\n","            avg_sentence_length,\n","            complex_word_count,\n","            word_count,\n","            avg_syllable_word_count,\n","            pp_count,\n","            average_word_length]\n","\n","#  write the values to the dataframe\n","# for i, var in enumerate(variables):\n","#      output_df.iloc[:,i+1] = var\n","\n","#now save the dataframe to the disk\n","output_df.to_csv('Output_Data.csv')"],"metadata":{"id":"PXTR2CXpaLiz","executionInfo":{"status":"ok","timestamp":1710193024249,"user_tz":-330,"elapsed":452,"user":{"displayName":"Krishna Singh Solanki","userId":"02860533251102802802"}}},"execution_count":295,"outputs":[]},{"cell_type":"code","source":["# Remove extra elements from the variables list\n","variables = variables[:output_df.shape[1] - 1]\n","\n","# Write the values to the DataFrame\n","for i, var in enumerate(variables):\n","    output_df.iloc[:, i+1] = var"],"metadata":{"id":"fDQ6L14QaYyb"},"execution_count":null,"outputs":[]}]}